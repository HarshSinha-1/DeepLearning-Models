{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Neural Networks for Computer Vision\n",
        "\n",
        "Convolutional neural networks (CNNs) is a different class of neural network.\n",
        "\n",
        "CNNs have been hugely successful in the domain of computer vision, and as we learn more about them, we will be able to appreciate the reasons for this.\n",
        "\n",
        "CNNs are a specialized kind of network that can take in images as tensors.\n",
        "\n",
        "A colored image consists of three color channels, red, green, and blue, referred to as RGB.\n",
        "\n",
        "These 2D-matrix channels are stacked to form colored images as we know them;\n",
        "\n",
        "the variations in the values of each channel give rise to different colors.\n",
        "\n",
        "A CNN takes in images as three separate stacked strata of color, one on top of the other.\n",
        "\n",
        "An image gets its meaning from a set pixel in the neighborhood, but a single pixel doesn't hold much information about the entire image.\n",
        "\n",
        "In a fully connected neural network, which is also called a dense layer, every node from one layer is connected to every other node in the subsequent layer.\n",
        "\n",
        "A CNN leverages the spatial structure between the pixels to reduce the number of connections between two layers, significantly improving the speed of training while at the same time reducing the model parameters.\n",
        "\n",
        "A CNN picks up features from an input image using a filter;\n",
        "\n",
        "a CNN with a sufficient number of filters detects various features in the image.\n",
        "\n",
        "These filters become more and more sophisticated in detecting complex features as we move more and more toward the later layers.\n",
        "\n",
        "Convolutional networks use these filters and map them one by one to create a map of feature occurrences."
      ],
      "metadata": {
        "id": "b9e90h-NugED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring convolutions\n",
        "\n",
        "Convolutions are a component within CNNs.\n",
        "\n",
        "They are defined as a layer within the CNNs.\n",
        "\n",
        "In a convolution layer, we slide a filter matrix over the entire image matrix from left to right and from top to bottom, and we take the dot product of the filter, with this patch spanning the size of the filter over the image channel.\n",
        "\n",
        "If the two matrices have high values in the same positions, the dot product's output will be high, and vice versa.\n",
        "\n",
        "The output of the dot product is a scalar value that identifies the correlation between the pixel pattern in the image and the pixel pattern expressed by the filter.\n",
        "\n",
        "Different filters detect different features from the image and at various levels of complexity.\n",
        "\n",
        "We need to understand two more key elements of CNNs, which are as follows:\n",
        "\n",
        "**Stride:** This is the number of pixels that we shift both horizontally and vertically before applying convolution networks using a filter on the next patch of the image.\n",
        "\n",
        "**Padding:** This is the strategy that we apply to the edges of an image while we convolve, depending on whether we want to keep the dimensions of the tensors the same after convolution or only apply convolution where the filter fits properly with the input image.\n",
        "If we want to keep the dimensions the same, then we need to zero pad the edge so that the original dimensions match with the output after convolution.\n",
        "\n",
        "This is called **same padding.**\n",
        "\n",
        "But if we don't want to preserve the original dimensions, then the places where the filter doesn't fit completely are truncated, which is called **valid padding.**"
      ],
      "metadata": {
        "id": "JOt-MvGptwSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "GoI6sHhCui73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will apply 2D convolution to an image\n",
        "\n",
        "nn.Conv2d(3, 16, 3, padding=1) #Creates a 2D convolution layer that takes 3 input channels, outputs 16 feature maps, uses 3×3 filters, and applies padding of 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmogZQcrulmg",
        "outputId": "df9a9ad1-72b7-4ef6-b41d-04fc6b4a528e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We then add striding and padding of the desired size to the edge of an image\n",
        "#Defines a 2D convolution layer with 3 input channels and 16 output feature maps using 3×3 kernels, a stride of 2 for downsampling, and asymmetric padding (1 row, 2 columns)\n",
        "nn.Conv2d(3, 16, 3, stride=2, padding=(1,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y81NxNe0uqo-",
        "outputId": "7ebbe2ea-9ec0-40fe-b999-3687582e6ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can then create a non square kernel (filter)\n",
        "#Creates a 2D convolution layer with 3 input channels and 16 output channels using rectangular kernels of size 3×4, and applies symmetric padding of 1\n",
        "nn.Conv2d(3, 16, (3,4), padding=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmvJQ7-musGa",
        "outputId": "346ec85f-9e2e-4ed6-a08d-9548365a5b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 4), stride=(1, 1), padding=(1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2D conv: 3 in, 16 out, 3×4 kernel, padding 1.\n",
        "nn.Conv2d(3, 16, (3,4), padding=(1,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab0qmF7uuyo2",
        "outputId": "f8f75401-5c19-434d-d55d-570824723cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 4), stride=(1, 1), padding=(1, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Conv2d(3, 16, (3,4), stride=(3,3), padding=(1,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QArQ6qZtu0iS",
        "outputId": "3b51ba2b-e779-4081-8b48-3bd46eb0a242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 4), stride=(3, 3), padding=(1, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Conv2d(3, 16, (3,4), stride=3, padding=(1,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZYwfR9Lu2KE",
        "outputId": "47b8d13a-58bd-4ef4-d1af-b62f6b71af85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 4), stride=(3, 3), padding=(1, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Conv2d(3, 16, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jrkdHaru3gV",
        "outputId": "f502448d-8d60-489f-d130-45623b3d10ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Conv2d(3, 16, 3, stride=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er3RMZePu48u",
        "outputId": "1eaae119-b602-4120-9aee-5340c626624c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We looked at multiple ways of creating a 2D convolution, wherein the first parameter is the number of channels in a given input image, which will be 3 for a color image and 1 for a grayscale image.\n",
        "\n",
        "The second parameter is the number of output channels—in other words, the number of filters that we want from the given layer.\n",
        "\n",
        "The third parameter is the kernel size—which is the size of the kernel—or the patch size of the image to be convoluted with a filter.\n",
        "\n",
        "We then created a Conv2d object and passed the input to the 2D convolutional layer to get the output.\n",
        "\n",
        "With nn.Conv2d(3, 16, 3), we created a convolutional layer, which takes in an input of 3 channels and outputs 16 channels.\n",
        "\n",
        "This layer has a square kernel of size 3 x 3 with a default stride of 1 in its height and width.\n",
        "\n",
        "We can add padding using the padding parameter, which can have an integer or a tuple value. Here, the integer value will create equal paddings for height and width, and a tuple value will have different paddings for height and width—this is true for the kernel size as well as the stride."
      ],
      "metadata": {
        "id": "dKWBPqgSu6Za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring pooling\n",
        "\n",
        "Now we move on to the next crucial layer of CNNs—the pooling layer.\n",
        "\n",
        "So far, we have been dealing with images without changing the spatial dimensions of the frames (considering the same padding); instead, we have been increasing the number of channels/filters.\n",
        "\n",
        "The pooling layer is used to reduce the spatial dimension of an input, preserving its depth.\n",
        "\n",
        "As we move from the initial layer to the later layers in a CNN, we want to identify more conceptual meaning in the image compared to actual pixel by pixel information, and so we want to identify and keep key pieces of information from the input and throw away the rest.\n",
        "\n",
        "A pooling layer helps us do that."
      ],
      "metadata": {
        "id": "oD0Hw-BQu-JS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here are the main reasons to use a pooling layer:\n",
        "\n",
        "**Reduction in the number of computations:** We get better computational performance by reducing the spatial dimensions of the input without losing out on the filters, and so we reduce the time needed to train, as well as the computational resources.\n",
        "\n",
        "Prevent overfitting: With reduced spatial dimensions, we reduce the number of parameters the model has, which in turn reduces the model complexity and helps us generalize better.\n",
        "\n",
        "**Positional invariance:** This allows the CNN to capture the features within an image, irrespective of where the feature is located in a given image. Say that we are trying to build a classifier to detect mangoes.\n",
        "\n",
        "It doesn't matter whether the mango is in the center, top-left, bottom-right, or wherever in the image—it needs to be detected.\n",
        "\n",
        "The pooling layer helps us with this.\n",
        "\n",
        "\n",
        "**There are many types of pooling, such as max pooling, average pooling, sum pooling, and so on;**\n",
        "\n",
        "however, max pooling is the most popular.\n",
        "\n",
        "In the same way that we dealt with a convolutional layer, we will define a window and apply the desired pooling operation in that window.\n",
        "\n",
        "We will slide the window horizontally and vertically, as defined by the stride of the layer."
      ],
      "metadata": {
        "id": "WjuppvfevANr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_pool = nn.MaxPool2d(3, stride=1) #Max pooling with 3×3 window and stride 1."
      ],
      "metadata": {
        "id": "7SNocbamu8Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.FloatTensor(3,5,5).random_(0, 10) #Creates a 3×5×5 float tensor with random integers 0–9."
      ],
      "metadata": {
        "id": "-vQ648yDvDhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqgEy-wTvEzt",
        "outputId": "e46cfdf0-6970-4021-a2f0-4af583ebc997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[8., 0., 9., 1., 2.],\n",
              "         [1., 6., 3., 2., 4.],\n",
              "         [4., 7., 6., 9., 5.],\n",
              "         [8., 4., 6., 5., 3.],\n",
              "         [7., 1., 7., 4., 2.]],\n",
              "\n",
              "        [[7., 7., 8., 0., 6.],\n",
              "         [6., 2., 2., 7., 0.],\n",
              "         [8., 0., 8., 1., 8.],\n",
              "         [5., 9., 2., 0., 1.],\n",
              "         [1., 2., 2., 6., 5.]],\n",
              "\n",
              "        [[8., 7., 6., 9., 8.],\n",
              "         [3., 2., 6., 7., 2.],\n",
              "         [8., 9., 4., 2., 2.],\n",
              "         [7., 0., 9., 8., 2.],\n",
              "         [2., 9., 6., 8., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_pool(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_9TH90AvGmH",
        "outputId": "0874ecc6-a015-42b1-c644-1778334c83ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[9., 9., 9.],\n",
              "         [8., 9., 9.],\n",
              "         [8., 9., 9.]],\n",
              "\n",
              "        [[8., 8., 8.],\n",
              "         [9., 9., 8.],\n",
              "         [9., 9., 8.]],\n",
              "\n",
              "        [[9., 9., 9.],\n",
              "         [9., 9., 9.],\n",
              "         [9., 9., 9.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_pool = nn.AvgPool2d(3, stride=1) #Average pooling with 3×3 window and stride 1.\n",
        "avg_pool(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_PTLHlgvIgP",
        "outputId": "ac2d7704-3837-4d99-999b-5ecba8a48e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[4.8889, 4.7778, 4.5556],\n",
              "         [5.0000, 5.3333, 4.7778],\n",
              "         [5.5556, 5.4444, 5.2222]],\n",
              "\n",
              "        [[5.3333, 3.8889, 4.4444],\n",
              "         [4.6667, 3.4444, 3.2222],\n",
              "         [4.1111, 3.3333, 3.6667]],\n",
              "\n",
              "        [[5.8889, 5.7778, 5.1111],\n",
              "         [5.3333, 5.2222, 4.6667],\n",
              "         [6.0000, 6.1111, 4.5556]]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We worked through an example of a tensor to see a pooling layer in action.\n",
        "\n",
        "We used a square kernel of size 3 x 3.\n",
        "\n",
        "The first application of pooling happened on the patch [0,0,0] to [0,3,3].\n",
        "\n",
        "Since the stride is 1, the next patch to be operated on was [0,0,1] to [0,3,4].\n",
        "\n",
        "Once it met the horizontal end, the tensor right below was operated on.\n",
        "\n",
        "Both nn.MaxPool2d(3, stride=1) and nn.AvgPool2d(3, stride=1) created a max and average pool square kernel of size 3x3 with a stride of 1, which was applied on a random tensor, a."
      ],
      "metadata": {
        "id": "9m5qcF8SvNGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring transforms\n",
        "\n",
        "PyTorch cannot process an image pixel directly and needs to have the contents as tensors.\n",
        "\n",
        "To get around this, torchvision, being a specialized library for vision and image-related tasks, provides a module called transform, which provides APIs for converting pixels into tensors, normalizing standard scaling, and so on. I\n",
        "\n",
        "Let us explore various methods in the transform module.\n",
        "\n",
        "torchvision must be installed"
      ],
      "metadata": {
        "id": "OAcq7SM7vO24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets"
      ],
      "metadata": {
        "id": "V7yt06PFvLxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "1F5bn_ljwzp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.Resize(1) #Resizes the input image so that its shorter side is scaled to 1 pixel while maintaining the aspect ratio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCslNHZUw2hp",
        "outputId": "1833fe32-fbbc-42c7-cd26-dd37861d6d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Resize(size=1, interpolation=bilinear, max_size=None, antialias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.Resize((1,1)) #Resizes the input image to exactly 1×1 pixels, forcing both height and width to 1 regardless of aspect ratio."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgwb8NY0w39I",
        "outputId": "5c0ad1b3-42c9-48a2-fa21-512e915dfc47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Resize(size=(1, 1), interpolation=bilinear, max_size=None, antialias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.Pad(1, 0) #Pads the image with a 1-pixel border on all sides using the value 0 (black)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u-BFLxkw5DU",
        "outputId": "1dca6e76-1c4c-4fc8-84e7-d74946c13079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pad(padding=1, fill=0, padding_mode=constant)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.Pad((1, 2), 1) #Pads the image with 1 pixel on left/right and 2 pixels on top/bottom using the value 1 (white or intensity 1)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR6auySzw6Xg",
        "outputId": "a1583a86-f0ae-4571-91c3-932f5684aafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pad(padding=(1, 2), fill=1, padding_mode=constant)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.Pad((1, 2, 2, 3), padding_mode='reflect') #Pads the image with 1 pixel left, 2 top, 2 right, and 3 bottom using reflected pixel values instead of a constant color."
      ],
      "metadata": {
        "id": "s0KViv9Kw8NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.Normalize((0.5,),(0.5,)) #Normalizes each channel by subtracting mean 0.5 and dividing by std 0.5, scaling values to roughly [-1, 1]."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOukZG_fw9cw",
        "outputId": "d7e369d5-610b-4e8a-e50a-1fbd2870a6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Normalize(mean=(0.5,), std=(0.5,))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we chain multiple transforms\n",
        "#Applies a center crop of size 10×10 to the image, then converts it to a PyTorch tensor.\n",
        "transforms.Compose([\n",
        "     transforms.CenterCrop(10),\n",
        "     transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7aikmsBw-ms",
        "outputId": "fa87f64b-cfd3-4dd9-ac6b-e3d0cfca3bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    CenterCrop(size=(10, 10))\n",
              "    ToTensor()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the preceding code snippets, we looked at the various transforms that are available in torchvision.\n",
        "\n",
        "These allow us to take input images and format them into tensors of the desired dimensions and properties, which can then be fed into torch models.\n",
        "\n",
        "The first method that we looked at was the toTensor() method, which transforms a given input image into a tensor.\n",
        "\n",
        "We could then normalize this input image tensor using the Normalize() method.\n",
        "\n",
        "The Normalize() method takes in two tuples, where the first tuple is the sequence of the means of each channel in the input image and the second tuple is the sequence of the standard deviation for each channel.\n",
        "\n",
        "Furthermore, we could resize a given image into the desired dimensions using the Resize() method, which, if given an integer, would match it with the length of the smaller edge, and if given a tuple, would match the height and width of the image.\n",
        "\n",
        "There would be certain cases where the crucial information regarding the image is in its center, and in such cases, it would be okay to crop and consider only the center of the given image; for this, you could use the CenterCrop() method.\n",
        "\n",
        "Then, we passed in an integer to crop a square from the center or to pass a sequence matching the height and width to CenterCrop().\n",
        "\n",
        "Another important task is to pad the image to match certain dimensions.\n",
        "\n",
        "For this, we use the Pad() method. We pass in the padding size as the integer for equal-sized padding on all sides or a sequence consisting of two elements for the padding size corresponding to the left/right and top/bottom, respectively.\n",
        "\n",
        "Furthermore, we could pass in the padding size for the left, top, right, and bottom sides as a sequence consisting of four elements.\n",
        "\n",
        "We then provided a fill value as an integer, and if it's a tuple of three elements, it's used as pad values for the R, G, and B channels, respectively.\n",
        "\n",
        "Apart from these, the Pad() method also has a padding_mode parameter, which takes in the following possibilities:\n",
        "\n",
        "constant: Pads with a fill value provided\n",
        "\n",
        "edge: Pads with the value at the edge of the image\n",
        "\n",
        "reflect: Pads with a reflection of the image, excluding the edge pixel\n",
        "\n",
        "symmetric: Pads with a reflection of the image, including the edge pixel\n",
        "\n",
        "In the end, we looked at the Compose() transform, which combined the various transforms to build a transformation pipeline by passing in a list of transform objects as a parameter."
      ],
      "metadata": {
        "id": "6dtsriT8xBrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.CenterCrop(10) #Crops the central 10×10 region of the image."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUINvHgxxAag",
        "outputId": "db129e39-ef4d-4bc7-bad7-d1b115155537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CenterCrop(size=(10, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performing data augmentation\n",
        "\n",
        "We will learn about data augmentation with torch.\n",
        "\n",
        "Data augmentation is an important technique in deep learning and computer vision.\n",
        "\n",
        "For any model dealing with deep learning or computer vision, the amount of data available is crucial to see how well the model performs.\n",
        "\n",
        "Data augmentation prevents models from memorizing the limited amount of data rather than making generalizations about the observed data.\n",
        "\n",
        "Data augmentation increases the diversity of data for training the model by creating variations from the original images without actually collecting new data.\n",
        "\n",
        "Oftentimes, the amount of light, brightness, orientation, or color variations doesn't make a difference to the inferences that are made by a model; however, when the model is deployed in the real world, the input data may have these variations.\n",
        "\n",
        "It is useful for the model to know that the decision it makes has to be invariant with respect to these variations in the input, and so data augmentation improves model performance."
      ],
      "metadata": {
        "id": "8w04AgdsxGGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.RandomCrop(10) #Randomly crops a 10×10 region from the image."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VxNNMnkxE8G",
        "outputId": "25012895-8891-4814-f2c4-d7937bf07fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomCrop(size=(10, 10), padding=None)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.RandomCrop((10, 20)) #Randomly crops a region of size 10×20 (height×width) from the image."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc8iPdhRxHyn",
        "outputId": "69dbda71-21fe-460e-c055-6326ee088715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomCrop(size=(10, 20), padding=None)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.RandomHorizontalFlip(p=0.3) #Randomly flips the image horizontally with a 30% probability."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlhvjn1hxI5w",
        "outputId": "0b1a47d9-aefb-4b39-e8e8-4031431eb267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomHorizontalFlip(p=0.3)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.RandomVerticalFlip(p=0.3) #Randomly flips the image vertically with a 30% probability."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB0bM6JDxKUS",
        "outputId": "401a1262-ffa1-4dd1-a412-28f0b6f2979a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomVerticalFlip(p=0.3)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.ColorJitter(0.25, 0.25, 0.25, 0.25) #Randomly changes brightness, contrast, saturation, and hue by up to ±25%."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8e53I2CxLdB",
        "outputId": "662e1f29-e7b5-4dac-c1b8-4c13ceb91b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ColorJitter(brightness=(0.75, 1.25), contrast=(0.75, 1.25), saturation=(0.75, 1.25), hue=(-0.25, 0.25))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.RandomRotation(10) #Randomly rotates the image by up to ±10 degrees."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cghq6ZO_xMnM",
        "outputId": "880e968a-a342-4cb5-e956-b4b580117e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Applies a random rotation up to ±10° to the image, then converts it to a PyTorch tensor.\n",
        "transforms.Compose([\n",
        "     transforms.RandomRotation(10),\n",
        "     transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtO5kFlmxOUt",
        "outputId": "ff4e0c6d-5421-4ae5-cb2f-a7f36bcf368e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    RandomRotation(degrees=[-10.0, 10.0], interpolation=nearest, expand=False, fill=0)\n",
              "    ToTensor()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Applies random horizontal flip, random rotation up to ±20°, converts to tensor, then normalizes each channel with mean 0.5 and std 0.5.\n",
        "transformations = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "PcGNR3kjxP1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw how we could add variations to our data by performing certain transformations that are meaningful for the problem at hand.\n",
        "\n",
        "We have to be careful when picking the right data augmentation that mimics the kind of image variations that we would get in real life.\n",
        "\n",
        "For instance, when building a car classifier, it would make sense to augment data with variations in colors and brightness, or flipping the car image horizontally, and so on; however, it would not make sense for us to augment data with car images that are flipped vertically, unless we are dealing with a problem where a car is turned upside down.\n",
        "\n",
        "We tried cropping the image in a random place so that if the entire image of an object isn't available but a portion is, then our model would be able to detect the object.\n",
        "\n",
        "We should include the cropped image size as an integer or a tuple of a particular height and width.\n",
        "\n",
        "Then, we flipped our image horizontally and passed in a probability for the random horizontal flip and vertical flip.\n",
        "\n",
        "We then created variations in the color, contrast, saturation, and hue of the image using the ColorJitter() method.\n",
        "\n",
        "We controlled the amount of variation in each of them by setting the parameter, where the color, contrast, and saturation vary between the [max(0, 1 - parameter), 1 + parameter] values and the hue varies between [-hue, hue], where the hue is between 0 and 0.5.\n",
        "\n",
        "We also added a random rotation to the images and provided the maximum angle of rotation.\n",
        "\n",
        "Finally, after we picked the right data augmentation strategy, we added it to transforms.compose()."
      ],
      "metadata": {
        "id": "9VUSuZjpxTxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**We will use the CIFAR-10 dataset, which consists of 60,000 32 x 32 pixel colored images for each of the 10 classes in the dataset.**\n",
        "\n",
        "**These classes are Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, and Truck.**"
      ],
      "metadata": {
        "id": "jF2Wc5WAxVq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.CIFAR10('CIFAR10', train=True, download=True, transform=transformations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9UawzE8xThM",
        "outputId": "74c77105-e631-4fe6-9fc4-2119a9f433c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 76.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = datasets.CIFAR10('CIFAR10', train=False, download=True, transform=transformations)"
      ],
      "metadata": {
        "id": "cteAB_uzxXvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JRyEFqrxY6h",
        "outputId": "7dee2c21-1d2a-467a-de11-6acce43ff15a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "metadata": {
        "id": "FCMh0Y8dxafH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_size = 0.2"
      ],
      "metadata": {
        "id": "m_AXAza-xbvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "0WuR7rRjxc9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_size = len(train_data)"
      ],
      "metadata": {
        "id": "vPlTSRIPxeIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = list(range(training_size))"
      ],
      "metadata": {
        "id": "TcYoP0SixfXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.shuffle(indices)"
      ],
      "metadata": {
        "id": "vCAqJlDzxgfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_split = int(np.floor(training_size * validation_size))"
      ],
      "metadata": {
        "id": "3G4Qckb4xhbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_indices, training_indices = indices[:index_split], indices[index_split:]"
      ],
      "metadata": {
        "id": "QxI5HiOJxi7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sample = SubsetRandomSampler(training_indices)\n",
        "validation_sample = SubsetRandomSampler(validation_indices)"
      ],
      "metadata": {
        "id": "fohKq-X4xkKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16"
      ],
      "metadata": {
        "id": "qSY3lU8DxlSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "metadata": {
        "id": "JMdWnrkExmkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, sampler=training_sample)\n",
        "valid_loader = DataLoader(train_data, batch_size=batch_size, sampler=validation_sample)\n",
        "test_loader = DataLoader(train_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "LrDlRD81xntN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code we used the datasets module in PyTorch to get the CIFAR10 dataset.\n",
        "\n",
        "We then defined the transformations that would make sense for the images in the dataset, which are images of animals corresponding to 10 different classes.\n",
        "\n",
        "We performed a horizontal flip for some of the images at random and also added rotation to some of the images at random, with a range of -20 to 20 degrees.\n",
        "\n",
        "However, we didn't add a vertical flip, since we don't anticipate having an upside-down image of animals to feed into the model in the evaluation phase.\n",
        "\n",
        "After that, we converted the images into tensors using the ToTensor() transform.\n",
        "\n",
        "Once the tensors were prepared, we used the Normalize() transform to set the mean and standard deviation for each of the red, green, and blue channels, respectively.\n",
        "\n",
        "Following this, we used the CIFAR10() method in the datasets to use the CIFAR10 dataset.\n",
        "\n",
        "Then, we set the download parameter to True so that if the dataset is not present in the root directory, CIFAR10 (the first argument), then it will be downloaded and kept in that directory.\n",
        "\n",
        "For the training data, we set the train parameter to True and passed the transformations that were to be applied to the data using the transform parameter.\n",
        "\n",
        "This allowed us to create images on the fly without explicitly creating new images.\n",
        "\n",
        "Now, to prepare the test data, we set the train argument to False. We set the size of the training and test dataset to be 50,000 and 10,000, respectively.\n",
        "\n",
        "Then, we prepared the validation set from the training set using 20% of the training set, as defined by validation_size.\n",
        "\n",
        "We randomly picked 20% of the training set to create a validation set so that the validation set is not skewed to a certain class of animal.\n",
        "\n",
        "We then took the size of the training set and prepared a list of indices using range() in Python.\n",
        "\n",
        "We then shuffled a list of indices using the random.shuffle() method in numpy.\n",
        "\n",
        "Once the list of indices was randomized, we moved the first 20% of the indices to the validation set and the remaining 80% of the indices to the training set.\n",
        "\n",
        "We found the split index by multiplying the original training size with the percentage of the original training set to be used as a validation set.\n",
        "\n",
        "We used split_index for the split.\n",
        "\n",
        "We then used the SubsetRandomSampler() method in torch.utils.data to sample the elements randomly from a given list of indices, without replacement.\n",
        "\n",
        "Finally, we used DataLoader() to combine a dataset and sampler to provide an iterable over the dataset.\n",
        "\n",
        "We then used the dataloader for the training, validation, and test sets to iterate over the data while training the model."
      ],
      "metadata": {
        "id": "VCHUNLy0xqFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the CNN architecture\n",
        "\n",
        "Now we will define the CNN model architecture from the components that we have seen so far to complete the model.\n",
        "\n",
        "This is very similar to the fully connected neural network."
      ],
      "metadata": {
        "id": "pAwANRwZxsin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "qRL7_ikyxo9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):  # Defines a custom CNN class inheriting from PyTorch's nn.Module.\n",
        "    def __init__(self): # Initializes the network layers.\n",
        "        super().__init__()# Calls the parent class constructor.\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1) # 1st conv: 3 input channels → 16 feature maps, 3x3 kernel, same padding.\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # 2nd conv: 16 input → 32 output channels, 3x3 kernel, same padding.\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1) # 3rd conv: 32 input → 64 output channels, 3x3 kernel, same padding.\n",
        "        self.pool = nn.MaxPool2d(2, 2) # Max pooling layer with 2x2 window and stride 2 to downsample.\n",
        "        self.linear1 = nn.Linear(64 * 4 * 4, 512) # Fully connected layer: flattens features to 512 units.\n",
        "        self.linear2 = nn.Linear(512, 10) # Output layer: 512 units → 10 classes\n",
        "        self.dropout = nn.Dropout(p=0.3)  # Dropout layer with 30% probability to reduce overfitting.\n",
        "\n",
        "    def forward(self, x): # Defines the forward pass.\n",
        "        x = self.pool(F.relu(self.conv1(x))) # Conv1 → ReLU → MaxPool.\n",
        "        x = self.pool(F.relu(self.conv2(x))) # Conv2 → ReLU → MaxPool.\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # Conv3 → ReLU → MaxPool.\n",
        "        x = x.view(-1, 64 * 4 * 4) # Flattens tensor to (batch_size, 1024).\n",
        "        x = self.dropout(x) # Applies dropout.\n",
        "        x = F.relu(self.linear1(x))  # Fully connected layer with ReLU activation.\n",
        "        x = self.dropout(x) # Applies dropout again.\n",
        "        x = self.linear2(x)  # Final output layer (logits for 10 classes).\n",
        "        return x   # Returns network output."
      ],
      "metadata": {
        "id": "VNl5Fb-wxuTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN()"
      ],
      "metadata": {
        "id": "G0GLt0pexwT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-teFgZ8Yxxst",
        "outputId": "dcd7532f-009f-4fe6-90ee-2ec75d100177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We defined a CNN class that is inherited from nn.Module in PyTorch by starting with the __init__() method and the constructor of the parent class.\n",
        "\n",
        "After that, we defined the various layers in our CNN by passing in the parameters relevant to each layer.\n",
        "\n",
        "For our first convolutional layer, the number of input channels was 3 (RGB), and the number of output channels was defined as 16 and had a square kernel size of 3.\n",
        "\n",
        "The second convolutional layer took in the tensors from the previous layer and had 16 input channels and 32 output channels with a kernel size of 3 x 3.\n",
        "\n",
        "Similarly, the third convolutional layer had 32 input channels and 64 output channels with a 3 x 3 kernel. We also needed a max pooling layer and used a kernel size of 2 and a stride of 2.\n",
        "\n",
        "We used .view() to flatten the three dimensions of the tensor into one dimension so that it could be passed into a fully connected network.\n",
        "\n",
        "The -1 in the view function ensured that the right size was automatically assigned to that dimension by making sure that the number of elements before and after the view function remained the same, which in this case was the batch size.\n",
        "\n",
        "For the first fully connected layer, we had 1,024 inputs (obtained from flattening the 64 x 4 x 4 tensor after the max pool) and 512 outputs.\n",
        "\n",
        "For the last fully connected layer, we had 512 inputs and 10 outputs, representing the number of output classes.\n",
        "\n",
        "We also defined a dropout layer for our fully connected layer with a probability of 0.3.\n",
        "\n",
        "Next, we defined the forward() method, where we wired together the components defined in the __init__() method.\n",
        "\n",
        "So, an input batch of 16 tensors, each with the dimensions of 32 x 32 x 3, went through the first convolutional layer, followed by a ReLU and then a max pooling layer, to form an output tensor with the dimensions of 16 x 16 x 16, and then through the second convolutional layer, followed by a ReLU and a max pool layer, with an output with the dimensions of 8 x 8 x 32, and then through the third convolutional layer, followed by a ReLU and a max pool layer, with the dimensions of 4 x 4 x 64.\n",
        "\n",
        "After this, we flattened the image out to a vector of 1,024 elements and passed it through the dropout layer into the first fully connected layer, giving us 512 outputs, followed by a ReLU and a dropout, into the final fully connected layer to give us the desired number of outputs, which is 10 in our case.\n",
        "\n",
        "We then instantiated the model from the CNN class and printed the model."
      ],
      "metadata": {
        "id": "DPgkwigqx0_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuHQRdMUxzAQ",
        "outputId": "06c9a0de-ebfc-4bd2-b150-58166d9381da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.12/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "CAUDjmRXx339"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "7cavDylmx50k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device.type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RjpMW0N7x7DH",
        "outputId": "5b8f0ecf-9b55-4c07-e04b-d215e35a2e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "EsfdDgIVx8NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "h6HagcSAx9O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "9VBqVviCx-co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "V3_JWQaDx_ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 10"
      ],
      "metadata": {
        "id": "ph8jTY2byCg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, y):\n",
        "    _, pred = torch.argmax(preds, 1)\n",
        "    correct = pred.eq(target.data.view_as(pred))\n",
        "    acc = correct.sum()/len(correct)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "CYm6z9OQyDzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "    model.eval()\n",
        "    for batch_idx, (data, target) in enumerate(valid_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f} |')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "php2oZ6MyFba",
        "outputId": "5493de3d-e1a3-4111-f827-27d338291260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch: 01 | Train Loss: 2.097 | Val. Loss: 1.836 |\n",
            "| Epoch: 02 | Train Loss: 1.687 | Val. Loss: 1.529 |\n",
            "| Epoch: 03 | Train Loss: 1.509 | Val. Loss: 1.418 |\n",
            "| Epoch: 04 | Train Loss: 1.413 | Val. Loss: 1.308 |\n",
            "| Epoch: 05 | Train Loss: 1.332 | Val. Loss: 1.246 |\n",
            "| Epoch: 06 | Train Loss: 1.265 | Val. Loss: 1.203 |\n",
            "| Epoch: 07 | Train Loss: 1.211 | Val. Loss: 1.149 |\n",
            "| Epoch: 08 | Train Loss: 1.162 | Val. Loss: 1.103 |\n",
            "| Epoch: 09 | Train Loss: 1.127 | Val. Loss: 1.052 |\n",
            "| Epoch: 10 | Train Loss: 1.092 | Val. Loss: 1.022 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"cifar10.pth\")"
      ],
      "metadata": {
        "id": "vWVIP9-RyHfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this, we made our imports and started by identifying and assigning our model to the appropriate device that we have on our machine.\n",
        "\n",
        "We used the model.to(device) method to move our model, which is more elegant than using model.cuda() or model.cpu().\n",
        "\n",
        "We then defined our loss function, also called criterion.\n",
        "\n",
        "Since this is a classification problem, we used cross-entropy loss.\n",
        "\n",
        "Then, we chose the SGD optimizer to update our model weights on backpropagation, with a learning rate of 0.01, and passed in the model parameters using model.parameters().\n",
        "\n",
        "We then ran our model for 30 epochs, though we could have chosen any reasonable number to do this.\n",
        "\n",
        "In the loop, we reset the training and validation losses to 0 and set the model in train mode, and then we iterated over each batch in the training dataset.\n",
        "\n",
        "We moved the batch first to the device so that, if we had limited GPU memory, not all the data would not be loaded fully into GPU memory.\n",
        "\n",
        "Then, we passed the input tensor into the model and fetched the output and passed it into the loss function to evaluate the difference in the labels that were predicted and the true labels.\n",
        "\n",
        "After this, we performed backpropagation using loss.backward() and updated the model weights using the optimizer.step() steps. We then aggregated the loss in the batch using total epoch loss. We then switched the model into the evaluations model using model.eval(), since the model's performance needed to be evaluated on the validation set and the model doesn't learn during this phase, and we needed to shut down the dropouts as well.\n",
        "\n",
        "Iterating over the validation batches, we got the model output and accumulated the losses across the validation batches in the entire epoch.\n",
        "\n",
        "After this, we formatted the model performance to see the model changes in each epoch.\n",
        "\n",
        "We noticed that the model training and validation losses decrease over the epochs, which is an indicator that the model is learning."
      ],
      "metadata": {
        "id": "tq3cCbOmyJEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random flip"
      ],
      "metadata": {
        "id": "V7iXRYa4yLhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load a sample image from the FashionMNIST dataset\n",
        "# We'll load the dataset without any initial transform, so we get a PIL Image.\n",
        "trainset_raw = datasets.FashionMNIST('Fashion_MNIST/', download=True, train=True, transform=None)\n",
        "\n",
        "# Get the first image and its label from the dataset\n",
        "example_pil_image, example_label = trainset_raw[0]\n",
        "class_name = trainset_raw.classes[example_label]\n",
        "\n",
        "print(f\"Original image (PIL) is a '{class_name}' (Label: {example_label})\")\n",
        "\n",
        "# 2. Define the RandomHorizontalFlip transform\n",
        "random_flip_transform = transforms.RandomHorizontalFlip(p=0.3)\n",
        "\n",
        "# 3. Define a transform to convert PIL Image to PyTorch Tensor for display\n",
        "# This also scales pixel values to [0.0, 1.0]\n",
        "to_tensor_for_display = transforms.ToTensor()\n",
        "\n",
        "# --- Displaying Original and Transformed Images ---\n",
        "\n",
        "num_flips_to_show = 6 # Number of times to apply the flip and display the result\n",
        "\n",
        "plt.figure(figsize=(12, 5)) # Adjust figure size for better display\n",
        "\n",
        "# Display the original image\n",
        "plt.subplot(2, (num_flips_to_show // 2) + 1, 1) # Dynamic subplot layout\n",
        "plt.imshow(example_pil_image, cmap='gray')\n",
        "plt.title(f\"Original\\n({class_name})\")\n",
        "plt.axis('off') # Hide axes ticks\n",
        "\n",
        "# Apply the random flip transform multiple times and display\n",
        "for i in range(num_flips_to_show):\n",
        "    # Apply the random flip transform to the original PIL image\n",
        "    flipped_pil_image = random_flip_transform(example_pil_image)\n",
        "\n",
        "    # Convert the (possibly) flipped PIL image to a NumPy array for matplotlib\n",
        "    # .squeeze() removes the channel dimension (1, H, W) -> (H, W) for grayscale\n",
        "    flipped_image_np = to_tensor_for_display(flipped_pil_image).squeeze().numpy()\n",
        "\n",
        "    # Create a subplot for each transformed image\n",
        "    plt.subplot(2, (num_flips_to_show // 2) + 1, i + 2) # +2 because original is at index 1\n",
        "    plt.imshow(flipped_image_np, cmap='gray')\n",
        "    plt.title(f\"Flip {i+1} (p=0.3)\")\n",
        "    plt.axis('off') # Hide axes ticks\n",
        "\n",
        "plt.suptitle(f\"Demonstration of transforms.RandomHorizontalFlip(p=0.3) on a '{class_name}'\", y=1.02, fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nExplanation:\")\n",
        "print(f\"We took an original image (a '{class_name}').\")\n",
        "print(f\"We then applied `transforms.RandomHorizontalFlip(p=0.3)` {num_flips_to_show} times.\")\n",
        "print(f\"This transform has a 30% chance (p=0.3) to horizontally flip the image each time it's applied.\")\n",
        "print(\"You will observe that some of the displayed images are horizontally flipped, while others are identical to the original, demonstrating the random nature of the transform.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "id": "xPxwbEZTyMq2",
        "outputId": "e44d8ba9-707f-472b-dedb-613460eb8804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 17.1MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 273kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.03MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 9.52MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original image (PIL) is a 'Ankle boot' (Label: 9)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 7 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation:\n",
            "We took an original image (a 'Ankle boot').\n",
            "We then applied `transforms.RandomHorizontalFlip(p=0.3)` 6 times.\n",
            "This transform has a 30% chance (p=0.3) to horizontally flip the image each time it's applied.\n",
            "You will observe that some of the displayed images are horizontally flipped, while others are identical to the original, demonstrating the random nature of the transform.\n"
          ]
        }
      ]
    }
  ]
}