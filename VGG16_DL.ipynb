{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I5xwzkr2TPFo"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BM63KP68S6C4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e185c4f3-3727-44da-b37f-7244164f9dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 105MB/s]\n"
          ]
        }
      ],
      "source": [
        "train_data = datasets.CIFAR10('CIFAR10', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tkRPRjVkTGs1"
      },
      "outputs": [],
      "source": [
        "test_data = datasets.CIFAR10('CIFAR10', train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvD3dLh2TIan",
        "outputId": "50291460-1500-413f-9490-3a6ef4a8deb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(train_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQtw3ZQNVmwg",
        "outputId": "a456407d-ca20-463e-abbc-34fa2206b8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 528M/528M [00:04<00:00, 122MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch: 01 | Train Loss: 2.116 | Val. Loss: 1.571 |\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --------------------------\n",
        "# Imports\n",
        "# --------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "\n",
        "# --------------------------\n",
        "# Dataset splitting\n",
        "validation_size = 0.2\n",
        "training_size = len(train_data)\n",
        "indices = list(range(training_size))\n",
        "np.random.shuffle(indices)\n",
        "index_split = int(np.floor(training_size * validation_size))\n",
        "\n",
        "validation_indices, training_indices = indices[:index_split], indices[index_split:]\n",
        "\n",
        "training_sample = SubsetRandomSampler(training_indices)\n",
        "validation_sample = SubsetRandomSampler(validation_indices)\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, sampler=training_sample)\n",
        "valid_loader = DataLoader(train_data, batch_size=batch_size, sampler=validation_sample)\n",
        "test_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "# --------------------------\n",
        "# Transfer Learning with VGG16\n",
        "# --------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pretrained VGG16 model\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "# Freeze convolutional layers (only train classifier)\n",
        "for param in vgg16.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify classifier for 10 classes\n",
        "num_features = vgg16.classifier[6].in_features\n",
        "vgg16.classifier[6] = nn.Linear(num_features, 10)\n",
        "\n",
        "model = vgg16.to(device)\n",
        "\n",
        "# --------------------------\n",
        "# Loss and Optimizer\n",
        "# --------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.classifier[6].parameters(), lr=0.001, momentum=0.9)\n",
        "n_epochs = 10\n",
        "\n",
        "# --------------------------\n",
        "# Accuracy function\n",
        "# --------------------------\n",
        "def accuracy(preds, y):\n",
        "    pred = preds.argmax(dim=1)\n",
        "    correct = pred.eq(y)\n",
        "    return correct.sum().item() / len(y)\n",
        "\n",
        "# --------------------------\n",
        "# Training + Validation Loop\n",
        "# --------------------------\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    train_loss, valid_loss = 0.0, 0.0\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, target in valid_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    train_loss = train_loss / len(train_loader.sampler)\n",
        "    valid_loss = valid_loss / len(valid_loader.sampler)\n",
        "\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f} |')\n",
        "\n",
        "# --------------------------\n",
        "# Testing the model\n",
        "# --------------------------\n",
        "model.eval()\n",
        "test_loss, test_acc = 0.0, 0.0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        test_loss += loss.item() * data.size(0)\n",
        "        test_acc += accuracy(output, target) * data.size(0)\n",
        "\n",
        "test_loss = test_loss / len(test_loader.sampler)\n",
        "test_acc = test_acc / len(test_loader.sampler)\n",
        "\n",
        "print(f'\\nTest Loss: {test_loss:.3f} | Test Accuracy: {test_acc:.3f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}